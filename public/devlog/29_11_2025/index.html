<!DOCTYPE html>
<html>
	<head id="head">
		<meta charset="utf-8">
		<meta name="viewport" content="initial-scale=1">
		<title>Welcome to my blog</title>
		<link type="text/css" rel="stylesheet" href="/style.css">
		<link type="text/css" rel="stylesheet" href="/highlight.css">
		<!-- mathtex -->
		<link type="text/css" rel="stylesheet" href="/Temml-Local.css">
		<script defer src="/temml.min.js"></script>
		<script defer src="/render-mathtex.js"></script>
		<!-- /mathtex -->
		
	<style>
		.feed {
			margin-top: 2em;
		}
		
		.feed>div {
			margin: 0 -20px;
			padding: 5px 20px;
			margin-bottom: 1em;
		}
		
		.feed>div>h2 {
			margin-top: 0.4em;
		}
		
		.feed>div[id]:target {
			animation: pulse-div 2s ease-in-out 1 forwards;
		}
		
		@keyframes pulse-div {
			
			0%,
			100% {
				background-color: #222;
			}
			
			50% {
				background-color: #111;
			}
		}
	</style>

	</head>
	<body id="body">
		<h1 class="site-title">Welcome to my blog</h1>
		<nav>
			<a href="/">Home</a>
			&nbsp; • &nbsp;
			<a href="/about/">About</a>
			&nbsp; • &nbsp;
			<a href="/blog/">Blog</a>
			
				&nbsp; • &nbsp;
				<a href="/devlog/29_11_2025/">Devlog</a>
			
		</nav>
		
	<h1 class="title"></h1>
	<div><div id="about"></div></div>
	<div class="feed">
		<div id="2025-11-30T00:00:00">
			<span>November 30, 2025</span>
			<h2><a href="#2025-11-30T00:00:00">Compression Queue
</a></h2>
			<p>When dealing with the blocks overlap problem, I can setup a queue from which jobs/blocks are popped. During the packing stage a pointer + start + end is popped onto the queue pointing to a block, we obtain a job then by popping from the queue till no overlap exists. We process the overlap and push back the blocks which are not completed. This approach is ok because we limit the packing to <code>k</code> many elements to limit cache access. One problem could be tho that we end up with a lot of fractured blocks at the end of the queue that cannot be processed together.</p><h2>Sleeping in spinlock</h2><p>Dealing With I/O And Sleeping Processes One problem with picking the lowest vruntime to run next arises with jobs that have gone to sleep for a long period of time. Imagine two pro- cesses, A and B, one of which (A) runs continuously, and the other (B) which has gone to sleep for a long period of time (say, 10 seconds). When B wakes up, its vruntime will be 10 seconds behind A’s, and thus (if we’re not careful), B will now monopolize the CPU for the next 10 sec- onds while it catches up, effectively starving A. CFS handles this case by altering the vruntime of a job when it wakes up. Specifically, CFS sets the vruntime of that job to the minimum value found in the tree (remember, the tree only contains running jobs) [B+18]. In this way, CFS avoids starvation, but not without a cost: jobs that sleep for short periods of time frequently do not ever get their fair share of the CPU [AC97]</p><h2>Some papers i want to read</h2><p><a href="https://arxiv.org/pdf/2402.18668" target="_blank">https://arxiv.org/pdf/2402.18668</a> Simple linear attention language models balance the recall-throughput tradeoff</p><p><a href="https://sites.cc.gatech.edu/fac/hyesoon/gputhread.pdf" target="_blank">https://sites.cc.gatech.edu/fac/hyesoon/gputhread.pdf</a> Effect of Instruction Fetch and Memory Scheduling on GPU Performance <a href="https://shreyansh26.github.io/post/2025-03-23_gtc25-maximize-memory-bandwidth-part-1/" target="_blank">https://shreyansh26.github.io/post/2025-03-23_gtc25-maximize-memory-bandwidth-part-1/</a> <a href="https://arxiv.org/pdf/2503.20481" target="_blank">https://arxiv.org/pdf/2503.20481</a> Analyzing Modern NVIDIA GPU cores</p><h2>Async Copies in Cuda</h2><p>Examining the statement from [ https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#asynchronous-copy-from-global-memory-to-shared-memory ]</p><pre><code>CUDA 11.0 introduces an async-copy feature that can be used within
device code to explicitly manage the asynchronous copying of data
from global memory to shared memory. This feature enables CUDA kernels 
to overlap copying data from global to shared memory with computation.
It also avoids an intermediary register file access traditionally present
between the global memory read and the shared memory write.
</code></pre><p>Specifically <code>This feature enables CUDA kernels to overlap copying data from global to shared memory with computation.</code> this statement suggests that with traditional synchronous copies such as:</p><pre><code>shared[tid] = global[tid]
</code></pre><p>computations cannot overlap. This is misleading in the sense that infact the warp will step through instructions for as long as it doesn’t use a register which will force the warp to be blocked.</p><p>Asynchronous copies avoid this register write, thus theoretically keeping registers free and allowing this warp to proceed with computations. <a href="https://forums.developer.nvidia.com/t/the-difference-between-asynchronous-copy-and-synchronous-copy/311861/7" target="_blank">https://forums.developer.nvidia.com/t/the-difference-between-asynchronous-copy-and-synchronous-copy/311861/7</a></p><p>Often times there is no productive work left to do for a warp after issuing an async load anyway which means it is stuck waiting for new data anyway and synchronizes on a barrier, in that case the only thing that gets hidden effectively is the mov instructions after to store the register data to shared memory.</p><p>This makes me wonder what the actual effects of a consumer producer split of warps is, if we use async instructions. We will effectivley only use a single threads to issue the load anyway and the consumer warp will not do anything else after. Wouldnt</p><p>Once a thread block is launched on a multiprocessor (SM), all of its warps are resident until their execution finishes. Thus a new block is not launched on an SM until there is sufficient number of free registers for all warps of the new block, and until there is enough free shared memory for the new block. <a href="https://en.wikipedia.org/wiki/Thread_block_(CUDA_programming)" target="_blank">https://en.wikipedia.org/wiki/Thread_block_(CUDA_programming)</a></p><h3>How cpu out of order works</h3><p><a href="https://en.wikipedia.org/wiki/Tomasulo%27s_algorithm" target="_blank">https://en.wikipedia.org/wiki/Tomasulo%27s_algorithm</a> <a href="https://ca.wikipedia.org/wiki/Reservation_station" target="_blank">https://ca.wikipedia.org/wiki/Reservation_station</a></p><h2>__syncwarp()</h2><p><code>__syncwarp()</code> was introduced in Volta as a result of the new Independent Thread Scheduling, pre Volta had one shared <code>PC</code> (Program Counter) and one shared Stack per warp. Diverging statements such as</p><pre><code>if (threadIdx.x &lt; 4){
    A; 
    B;
}
else {
    X;
    Y;
}
Z;
</code></pre><p>were forced to execute as:</p><pre><code>      X-&gt;Y  |Z
A -&gt;B       |Z
</code></pre><p>This means threads reconverge when the program counter reaches the location of <code>Z</code>, this is problematic if <code>Z</code> contained a call necessary to complete the divergence, consider the example of locking a mutex, using atomics all but one thread in a warp would see the mutex as unlocked and subsequently lock it, while the other threads would see a locked mutex and wait. So if <code>Z</code> was the call to unlock the mutex, then this would permanently deadlock this warp.</p><p>With Volta each thread within a warp maintains their own program counter and stack, allowing interleaved execution of diverging threads:</p><pre><code>    X    Y         Z
A     B       Z
</code></pre><p><code>Z</code> is not an implicit reconvergence point anymore, although the scheduler may identify it is safe to reconvergen on it anyway. Programmers can now use <code>__syncwarp()</code> to explicitly mark <code>PC</code> value at which warps are forced to reconverge, this means that</p><pre><code>if (threadIdx.x &lt; 4){
    A; 
    B;
}
else {
    X;
    Y;
}
Z;
__syncwarp()
</code></pre><p>will force synchronization after <code>Z</code>, this is the correct behavior for the mutex example. If we know it is safe to execute <code>Z</code> as one, then we can swap their order</p><pre><code>__syncwarp()
Z;
</code></pre><p>yielding the following execution :</p><pre><code>    X    Y    | Z
A     B       | Z
</code></pre><p>Although when using warp level primitives I think that synchronization behaviour is not implied in the sense that while all threads that can reach <code>__syncwarp()</code> will be in sync at that point, this does not necessairly imply that they will be in sync after. <a href="https://developer.nvidia.com/blog/using-cuda-warp-level-primitives/" target="_blank">https://developer.nvidia.com/blog/using-cuda-warp-level-primitives/</a></p><pre><code>Calling the new __syncwarp() primitive at line 10 before __ballot(), as illustrated in Listing 11, does not fix the problem either. This is again implicit warp-synchronous programming. It assumes that threads in the same warp that are once synchronized will stay synchronized until the next thread-divergent branch. Although it is often true, it is not guaranteed in the CUDA programming model.
</code></pre><p>Effectively after Volta (or only on Volta ? Should be all after. <a href="https://docs.nvidia.com/cuda/hopper-compatibility-guide/" target="_blank">https://docs.nvidia.com/cuda/hopper-compatibility-guide/</a> ) no implicit assumptions should be made about where warps may diverge since they do not share a program counter. This means that threads even within a warp may race ahead even in situations where this is not expected. This means that code such as:</p><pre><code>      volatile __shared__ int data[64];
      data[threadIdx.x] = in[threadIdx.x];

      data[threadIdx.x] += data[threadIdx.x + 16];
      data[threadIdx.x] += data[threadIdx.x + 8];
      data[threadIdx.x] += data[threadIdx.x + 4];
      data[threadIdx.x] += data[threadIdx.x + 2];
      data[threadIdx.x] += data[threadIdx.x + 1];

      //data[0] should be sum of in[0..31]
       out[threadIdx.x] = data[0];
</code></pre><p>Is wrong on two levels, first we assume that the warp indeed has proceeded in lockstep until <code>out[threadIdx.x] = data[0]</code> which may not be true. And second we make an assumption about the memory ordering, the cuda programmign modle assumes a weakly ordered memory model, this means no Release or Acquire semantics for loads and stores.</p><pre><code>                    Release
Acquire #LoadLoad   #LoadStore
        #StoreLoad  #StoreStore
</code></pre><p><a href="https://preshing.com/20120913/acquire-and-release-semantics/" target="_blank">https://preshing.com/20120913/acquire-and-release-semantics/</a> <code>#XY</code> means the reordering of <code>Y</code> before <code>X</code>, Acquire semantics apply to load operations only and imply that no memory operation after may be re ordered to happen before it, Release does the same but in the opposite direction, no load or store before the release may happen to appear after the release. Acquire and Release semantics can be implemented using memory barriers, which prevent reordering accross it and force store-buffers to be flushed See: <a href="http://www.rdrop.com/users/paulmck/scalability/paper/whymb.2010.07.23a.pdf" target="_blank">http://www.rdrop.com/users/paulmck/scalability/paper/whymb.2010.07.23a.pdf</a> or <code>memory-barriers.txt</code> in the linux kernel documentation.</p><p>In cuda, memory fences are provided to enforce a sequentially-consistent ordering on memory accesses. <code>__threadfence_block()</code> enforces that all writes made by the calling thread prior to the fence are observedy by all threads in the block of the calling thread as occuring before all writes to all memory made by the calling thread after the call to <code>__threadfence_block()</code>.</p><p>And: All reads from memory made by the calling thread prior to the call to <code>__threadfence_block()</code> are ordered before all reads from memory made by the calling thread after the call to <code>__threadfence_block()</code></p><p>This apparently does not prevent <code>#StoreLoad</code> reordering, although I guess that might not be a problem here. Furthermore I wonder if we really cannot rely on program order, I guess what voltas execution model implies is that while on a thread level we can rely on program order, we cannot rely on it on a warp level anymore. On a thread level there will be no out-of-order processing, but on a warp level that may as well be the case.</p><pre><code>GPU vs CPU
Unlike a CPU core which can occupy 4mm² a CUDA core takes up less than 0.1mm² (at the latest 4NP node), a much smaller die area.
Because of this, the GPU cores do not have out-of-order execution, cache predictions, prefetching of data etc.
Whatever order the compiler puts the instructions in is the order in which they get executed.
``` From [https://stackoverflow.com/questions/79429440/cuda-memory-model-why-acquire-fence-is-not-needed-to-prevent-load-load-reorderi](https://stackoverflow.com/questions/79429440/cuda-memory-model-why-acquire-fence-is-not-needed-to-prevent-load-load-reorderi) seems to suggest 
that we can rely on program order in some sense, this seems to conflict with the volta execution model.

[https://stackoverflow.com/questions/69606996/cuda-lane-id-vs-threadidx-x-based-computation](https://stackoverflow.com/questions/69606996/cuda-lane-id-vs-threadidx-x-based-computation)
Here another post about this.


Although the semantics of `__threadfence_block` do not seems to prevent compiler reordering of `#StoreLoad` at all, 
why is: 
</code></pre><p>smem[y1][x1] = val; __syncwarp(); val = smem[y2][x2];</p><pre><code>valid code? There is seemingly no connection between this write and read from shared memory.
Looking at the ptx it wont reorder them and `__syncwarp` guarantees sequential ordering around it, i.e. 
is a full blown barrier.

I guess [https://docs.nvidia.com/cuda/parallel-thread-execution/#parallel-synchronization-and-communication-instructions-membar](https://docs.nvidia.com/cuda/parallel-thread-execution/#parallel-synchronization-and-communication-instructions-membar)
clears this up, membar actively prevents reordering of any memory operations around it, i.e. is a full blown fence.


</code></pre><p>8.9. Ordering of memory operations</p><p>The sequence of operations performed by each thread is captured as program order while memory synchronization across threads is captured as causality order. The visibility of the side-effects of memory operations to other memory operations is captured as communication order. The memory consistency model defines contradictions that are disallowed between communication order on the one hand, and causality order and program order on the other. 8.9.1. Program Order</p><p>The program order relates all operations performed by a thread to the order in which a sequential processor will execute instructions in the corresponding PTX source. It is a transitive relation that forms a total order over the operations performed by the thread, but does not relate operations from different threads.</p><pre><code>[https://forums.developer.nvidia.com/t/preventing-ptxas-from-reordering-instructions/28682/13](https://forums.developer.nvidia.com/t/preventing-ptxas-from-reordering-instructions/28682/13)


`bar.arrive $barnum $threadcount`, bar.arrive can be used to reset a barrier, and thus 
threadcount indicates how many threads the new barrier must wait on.









</code></pre>
		</div>
	</div>

		<!-- <footer> -->
		<!-- 	<hr> -->
		<!-- 	<h5>SITE UNDER CONSTRUCTION</h5> -->
		<!-- 	<img src="$site.asset('under-construction.gif').link()" width=90> -->
		<!-- </footer> -->
	</body>
</html>

---
.title = "A mental model for understanding parallel concepts",
.date = @date("2025-11-29T00:00:00"),
.author = "Janik ",
.layout = "post.shtml",
.draft = false,
--- 
The biggest issue I faced when trying to understand asynchronous code, was 
how it was possible for a function to stop/yield when encountering an `await` 
on a future that couldn't progress.

Conceptually I understood that an `await fn` gets translated into 
some state machine and it somehow manages to resume by proceeding through the steps.
But that is more accepting that it somehow works, not true understanding where I could 
implement it myself. 

The first moment of real understanding came when researching how to do something 
similar in C and I came across [Duff's device](https://en.wikipedia.org/wiki/Duff%27s_device).
```c

    register n = (count + 7) / 8;
    switch (count % 8) 
    {
    case 0: do { *to = *from++;
    case 7:      *to = *from++;
    case 6:      *to = *from++;
    case 5:      *to = *from++;
    case 4:      *to = *from++;
    case 3:      *to = *from++;
    case 2:      *to = *from++;
    case 1:      *to = *from++;  
            } while (--n > 0);
    }
```
Whenever `count` is not a multiple of 8, it simply jumps to the correct start of the 
iteration to handle any potential remainder and otherwise proceeds as normal.

Duff's device leverages the fact that in C switch statements the following properties hold:
- A case label can prefix any sub-statement within the switches body, as long as the body as a whole is a valid statement.
- Execution will fall through the case statements until it encounters a break or the switch ends.

Before proceeding, let a coroutine be any function that can be suspended and resumed.
Using Duff's device we can then implement a simple coroutine as follows:
```c
int co(){
    static int i,state = 0;
    switch (state) {
        case 0:
        for (i = 0; i < 10; ++i){
            state = 1;
            return i;
            case 1:;
        }
    }
}
``` 
Source:[https://www.chiark.greenend.org.uk/~sgtatham/coroutines.html](https://www.chiark.greenend.org.uk/~sgtatham/coroutines.html)


What you should notice, is that if you want some local variable to survive the suspension, 
then it has to be explicitly saved. In the example above we simply declared it as static, but in practice we would 
save any local state inside of a context struct and pass it to the coroutine:
```c
struct Context; 
// Access persitent variables with ctx->var
int co(Context *ctx);
```
# Context 
Context to me is also the most important attribute to distinguish and 
understand different parallel constructs. By looking at what context 
they require to function, we can easily distinguish between different types.

## Stackfull vs Stackless
When talking about coroutines people generally differentiate between 
stackfull-coroutines (fibers) and stackless-coroutines. Without going into
any pros or cons, we can reduce their differences 
to the fact that a stackfull-coroutines will include a stack in its context:
```c
struct Context {
    void *stack;
    // rest
};
```
while a stackless coroutine won't.
This difference is what makes stackless coroutines cheaper compared to stackfull ones 
since they require less space.

## Threads vs. Processes
Once again to distinguish threads from processes we can reduce the two 
to their differences in **context**. A process for example has its own 
virtual address space, while threads within a process share a common one.
This mean we don't have to save any information relating to that, such as the page table, 
when swapping between threads within a process. 

>[]($block.attrs('note'))
>The exact context used for processes and threads can be found by taking a look into `include/linux/sched.h` and finding the `task_struct`. 
>
>Processor specific information, like the `thread_struct` can be found in `arch/*/include/asm/processor.h`.

## GPU Threads vs. CPU Threads
Why is it that a GPU can run thousands of threads at the same time, while a CPU is limited 
to a small few? One core difference is the availability of a large amount of registers 
on the GPU. This allows thread context to be stored in registers rather 
than global memory like on the CPU.
And Switching between different threads
is then as simple as pointing to the relevant registers.

While another obvious difference is the number of execution units available on the GPU 
compared to the CPU, it isn't like the CPU doesn't have multiple execution units per core aswell.

So if one wanted to we could also run multiple "threads" on 
one CPU core at the same time by:
- Sharing a stack among all "threads".
- And limiting the context of each "thread" to one or two registers.

Whether:
```c

int acc1 = 0;
int acc2 = 0;
for (int i = 0 ; i < 2*n ; i+= 2){
    acc1 += A[i];
    acc2 += A[i+1];
}
```
this is one thread performing two adds at the same time, or it is two "threads"
each with one register `acc` performing one add at a time is just a matter of perspective.

To summarize: 
>[]($block.attrs('theorem'))
>Different parallel constructs can be reduced to the type of context 
>they require to execute, and their functionality is limited by
> the context they have access to.

# No Free Lunch

The next part that was never quite clear to me, is what is the `async` function 
even yielding to. A lot of `async` implementations hide the underlying mechanism 
of how `async` functions get run/scheduled. In rust for example you import tokio,
declare your new async main and somehow it all works out.
To me it seemed like there was some magic running in the background that knew exactly when 
to pick a task back up the moment data arrived.

The reality is, that what is abstracted away is some thread pool / userspace-scheduler 
that is responsible for picking which tasks to run. They then assign them to some underlying 
kernel thread and if necessary put them back into the queue if the task didn't run 
till completion. 

The core component that makes all of this work, ignoring everything else, 
is the fact that there is some component that is spinning in an endless 
loop, checking for some condition to be true.
Whether that is the *tokio* thread pool checking for new tasks, or the linux scheduler looking 
for new threads to schedule, as long as you keep peeling away layers 
of abstraction you will find something that is spinning.

>[]($block.attrs('note'))
>The exact mechanics of how the scheduler works don't really matter for this point. 
>But one can take a look at `linux/kernel/sched/idle.c` and find:
>```c 
>void cpu_startup_entry(enum cpuhp_state state)
>{
>	current->flags |= PF_IDLE;
>	arch_cpu_idle_prepare();
>	cpuhp_online_idle(state);
>	while (1)
>		do_idle();
>}
>```

There is no zero cost magic that somehow knows when to do work and otherwise turns itself 
off. And sure, relying on the kernel to tell us when work can be done is alot more 
effective than doing it ourselves. But it is not free nontheless.



# What this all means

By simply combining the two concepts of **context** and **spinning** we can cobble together 
a simple async runtime. 
```c
typedef struct Task {
    // Context required by the task
    void *context; 
    // Returns 1 if it needs to be scheduled again and otherwise 0 
    int (*run)(void *);
} Task;

// Some task queue with some fixed capacity
TaskQueue task_list; 

int main(){
    while (1){
        if (!queueEmpty(&task_list)){
            Task task = queuePop(&task_list);
            int notcompleted = task.run(task.context);
            if (notcompleted && !queueFull(&task_list)){
                queuePush(&task_list,task);
            }
        }
    } 
}
```
[^1]

But thinking about things in terms of their **context**, has uses beyond scheduling.
Turning a recursive function into an iterative one for example, can be done 
by explicitly packaging up the function together with the context it needs 
and pushing them onto our own stack. How complicated that stack needs to be, 
simply depends on what context the function needs.



[^1]: This shouldnt be considered a working example, if you want to look 
at real implementations of thread pools I recommed 
[https://github.com/kprotty/zap](https://github.com/kprotty/zap)
which even comes with it's own blog post explaining how it works.
And if you look close enough, you will find both our Task struct 
and a spinning loop





## References: 
- [https://en.wikipedia.org/wiki/Duff%27s_device](https://en.wikipedia.org/wiki/Duff%27s_device)
- [https://www.chiark.greenend.org.uk/~sgtatham/coroutines.html](https://www.chiark.greenend.org.uk/~sgtatham/coroutines.html)
- [https://github.com/kprotty/zap](https://github.com/kprotty/zap)







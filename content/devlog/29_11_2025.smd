---
.title = "",
.date = @date("2025-11-30T00:00:00"),
.author = "Sample Author",
.layout = "devlog.shtml",
.draft = false,
--- 
[]($section.id('about'))

## [Compression Queue]($section.id("2025-11-30T00:00:00"))
When dealing with the blocks overlap problem, I can setup 
a queue from which jobs/blocks are popped. During the packing 
stage a pointer + start + end is popped onto the queue pointing to a 
block, we obtain a job then by popping from the queue till no overlap 
exists. We process the overlap and push back the blocks which are not completed.
This approach is ok because we limit the packing to `k` many elements 
to limit cache access. One problem could be tho that we end up with a lot 
of fractured blocks at the end of the queue that cannot be processed together.



## Sleeping in spinlock
Dealing With I/O And Sleeping Processes
One problem with picking the lowest vruntime to run next arises with
jobs that have gone to sleep for a long period of time. Imagine two pro-
cesses, A and B, one of which (A) runs continuously, and the other (B)
which has gone to sleep for a long period of time (say, 10 seconds). When
B wakes up, its vruntime will be 10 seconds behind A's, and thus (if
we're not careful), B will now monopolize the CPU for the next 10 sec-
onds while it catches up, effectively starving A.
CFS handles this case by altering the vruntime of a job when it wakes
up. Specifically, CFS sets the vruntime of that job to the minimum value
found in the tree (remember, the tree only contains running jobs) [B+18].
In this way, CFS avoids starvation, but not without a cost: jobs that sleep
for short periods of time frequently do not ever get their fair share of the
CPU [AC97]


## Some papers i want to read
[https://arxiv.org/pdf/2402.18668](https://arxiv.org/pdf/2402.18668)
Simple linear attention language models balance the recall-throughput tradeoff

[https://sites.cc.gatech.edu/fac/hyesoon/gputhread.pdf](https://sites.cc.gatech.edu/fac/hyesoon/gputhread.pdf)
Effect of Instruction Fetch and Memory Scheduling on GPU Performance
[https://shreyansh26.github.io/post/2025-03-23_gtc25-maximize-memory-bandwidth-part-1/](https://shreyansh26.github.io/post/2025-03-23_gtc25-maximize-memory-bandwidth-part-1/)
[https://arxiv.org/pdf/2503.20481](https://arxiv.org/pdf/2503.20481)
Analyzing Modern NVIDIA GPU cores
## Async Copies in Cuda
Examining the statement from [ https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#asynchronous-copy-from-global-memory-to-shared-memory ]
```
CUDA 11.0 introduces an async-copy feature that can be used within
device code to explicitly manage the asynchronous copying of data
from global memory to shared memory. This feature enables CUDA kernels 
to overlap copying data from global to shared memory with computation.
It also avoids an intermediary register file access traditionally present
between the global memory read and the shared memory write.
```
Specifically `This feature enables CUDA kernels to overlap copying data from global to shared memory with computation.`
this statement suggests that with traditional synchronous copies such as:
```
shared[tid] = global[tid]
```
computations cannot overlap. This is misleading in the sense that infact the 
warp will step through instructions for as long as it doesn't use a register 
which will force the warp to be blocked. 

Asynchronous copies avoid this register write, thus theoretically keeping registers 
free and allowing this warp to proceed with computations.
[https://forums.developer.nvidia.com/t/the-difference-between-asynchronous-copy-and-synchronous-copy/311861/7](https://forums.developer.nvidia.com/t/the-difference-between-asynchronous-copy-and-synchronous-copy/311861/7)

Often times there is no productive work left to do for a warp after issuing an async load anyway
which means it is stuck waiting for new data anyway and synchronizes on a barrier, in that case the only 
thing that gets hidden effectively is the mov instructions after to store the register data to shared memory.

This makes me wonder what the actual effects of a consumer producer split of warps is, if we use async instructions.
We will effectivley only use a single threads to issue the load anyway and the consumer warp will not do anything else after.
Wouldnt 


Once a thread block is launched on a multiprocessor (SM), all of its warps are resident until their execution finishes. Thus a new block is not launched on an SM until there is sufficient number of free registers for all warps of the new block, and until there is enough free shared memory for the new block. 
[https://en.wikipedia.org/wiki/Thread_block_(CUDA_programming)](https://en.wikipedia.org/wiki/Thread_block_(CUDA_programming))

### How cpu out of order works 
[https://en.wikipedia.org/wiki/Tomasulo%27s_algorithm](https://en.wikipedia.org/wiki/Tomasulo%27s_algorithm)
[https://ca.wikipedia.org/wiki/Reservation_station](https://ca.wikipedia.org/wiki/Reservation_station)




## __syncwarp()
`__syncwarp()` was introduced in Volta as a result of the new Independent Thread Scheduling,
pre Volta had one shared `PC` (Program Counter) and one shared Stack per warp. Diverging 
statements such as 
```
if (threadIdx.x < 4){
    A; 
    B;
}
else {
    X;
    Y;
}
Z;
```
were forced to execute as: 
```
      X->Y  |Z
A ->B       |Z
```
This means threads reconverge when the program counter reaches the location of `Z`,
this is problematic if `Z` contained a call necessary to complete the divergence, 
consider the example of locking a mutex, using atomics all but one thread in a warp would 
see the mutex as unlocked and subsequently lock it, while the other threads would see a locked 
mutex and wait. So if `Z` was the call to unlock the mutex, then this would permanently deadlock 
this warp.


With Volta each thread within a warp maintains their own program counter and stack, allowing 
interleaved execution of diverging threads: 
```
    X    Y         Z
A     B       Z
```
`Z` is not an implicit reconvergence point anymore, although the scheduler 
may identify it is safe to reconvergen on it anyway. Programmers 
can now use `__syncwarp()` to explicitly mark `PC` value at which 
warps are forced to reconverge, this means that 
```
if (threadIdx.x < 4){
    A; 
    B;
}
else {
    X;
    Y;
}
Z;
__syncwarp()
```
will force synchronization after `Z`, this is the correct behavior for the mutex example. 
If we know it is safe to execute `Z` as one, then we can swap their order 
```
__syncwarp()
Z;
```
yielding the following execution : 
```
    X    Y    | Z
A     B       | Z
```
Although when using warp level primitives I think that synchronization behaviour 
is not implied in the sense that while all threads that can reach `__syncwarp()`
will be in sync at that point, this does not necessairly imply that they will 
be in sync after. [https://developer.nvidia.com/blog/using-cuda-warp-level-primitives/](https://developer.nvidia.com/blog/using-cuda-warp-level-primitives/)
```
Calling the new __syncwarp() primitive at line 10 before __ballot(), as illustrated in Listing 11, does not fix the problem either. This is again implicit warp-synchronous programming. It assumes that threads in the same warp that are once synchronized will stay synchronized until the next thread-divergent branch. Although it is often true, it is not guaranteed in the CUDA programming model.
```

Effectively after Volta (or only on Volta ? Should be all after. [https://docs.nvidia.com/cuda/hopper-compatibility-guide/](https://docs.nvidia.com/cuda/hopper-compatibility-guide/) ) no implicit 
assumptions should be made about where warps may diverge since they do not share a program counter. This means that threads even within a warp 
may race ahead even in situations where this is not expected. This means that code such as: 
```
      volatile __shared__ int data[64];
      data[threadIdx.x] = in[threadIdx.x];

      data[threadIdx.x] += data[threadIdx.x + 16];
      data[threadIdx.x] += data[threadIdx.x + 8];
      data[threadIdx.x] += data[threadIdx.x + 4];
      data[threadIdx.x] += data[threadIdx.x + 2];
      data[threadIdx.x] += data[threadIdx.x + 1];

      //data[0] should be sum of in[0..31]
       out[threadIdx.x] = data[0];
```
Is wrong on two levels, first we assume that the warp indeed has proceeded in lockstep until `out[threadIdx.x] = data[0]` which may not be true. 
And second we make an assumption about the memory ordering, the cuda programmign modle assumes 
a weakly ordered memory model, this means no Release or Acquire semantics for loads and stores.
```
                    Release
Acquire #LoadLoad   #LoadStore
        #StoreLoad  #StoreStore
```
[https://preshing.com/20120913/acquire-and-release-semantics/](https://preshing.com/20120913/acquire-and-release-semantics/)
`#XY` means the reordering of `Y` before `X`, Acquire semantics apply to load operations only 
and imply that no memory operation after may be re ordered to happen before it, Release does the same 
but in the opposite direction, no load or store before the release may happen to appear after the release.
Acquire and Release semantics can be implemented using memory barriers, which prevent reordering accross it and force store-buffers to be flushed
See: [http://www.rdrop.com/users/paulmck/scalability/paper/whymb.2010.07.23a.pdf](http://www.rdrop.com/users/paulmck/scalability/paper/whymb.2010.07.23a.pdf) or `memory-barriers.txt` in the linux kernel documentation.


In cuda, memory fences are provided to enforce a sequentially-consistent ordering on memory 
accesses. `__threadfence_block()` enforces that all writes made by the calling thread prior to the fence 
are observedy by all threads in the block of the calling thread as occuring before all writes to all memory made by the 
calling thread after the call to `__threadfence_block()`. 

And: All reads from memory made by the calling thread prior to the call to `__threadfence_block()` are ordered before 
all reads from memory made by the calling thread after the call to `__threadfence_block()` 

This apparently does not prevent `#StoreLoad` reordering, although I guess that might not be a problem here. 
Furthermore I wonder if we really cannot rely on program order, I guess what voltas execution model implies 
is that while on a thread level we can rely on program order, we cannot rely on it on a warp level anymore.
On a thread level there will be no out-of-order processing, but on a warp level that may as well be the case.

```
GPU vs CPU
Unlike a CPU core which can occupy 4mm² a CUDA core takes up less than 0.1mm² (at the latest 4NP node), a much smaller die area.
Because of this, the GPU cores do not have out-of-order execution, cache predictions, prefetching of data etc.
Whatever order the compiler puts the instructions in is the order in which they get executed.
``` From [https://stackoverflow.com/questions/79429440/cuda-memory-model-why-acquire-fence-is-not-needed-to-prevent-load-load-reorderi](https://stackoverflow.com/questions/79429440/cuda-memory-model-why-acquire-fence-is-not-needed-to-prevent-load-load-reorderi) seems to suggest 
that we can rely on program order in some sense, this seems to conflict with the volta execution model.

[https://stackoverflow.com/questions/69606996/cuda-lane-id-vs-threadidx-x-based-computation](https://stackoverflow.com/questions/69606996/cuda-lane-id-vs-threadidx-x-based-computation)
Here another post about this.


Although the semantics of `__threadfence_block` do not seems to prevent compiler reordering of `#StoreLoad` at all, 
why is: 
```
smem[y1][x1] = val;
__syncwarp();
val = smem[y2][x2];
```
valid code? There is seemingly no connection between this write and read from shared memory.
Looking at the ptx it wont reorder them and `__syncwarp` guarantees sequential ordering around it, i.e. 
is a full blown barrier.

I guess [https://docs.nvidia.com/cuda/parallel-thread-execution/#parallel-synchronization-and-communication-instructions-membar](https://docs.nvidia.com/cuda/parallel-thread-execution/#parallel-synchronization-and-communication-instructions-membar)
clears this up, membar actively prevents reordering of any memory operations around it, i.e. is a full blown fence.


```
8.9. Ordering of memory operations

The sequence of operations performed by each thread is captured as program order while memory synchronization across threads is captured as causality order. The visibility of the side-effects of memory operations to other memory operations is captured as communication order. The memory consistency model defines contradictions that are disallowed between communication order on the one hand, and causality order and program order on the other.
8.9.1. Program Order

The program order relates all operations performed by a thread to the order in which a sequential processor will execute instructions in the corresponding PTX source. It is a transitive relation that forms a total order over the operations performed by the thread, but does not relate operations from different threads.
```
[https://forums.developer.nvidia.com/t/preventing-ptxas-from-reordering-instructions/28682/13](https://forums.developer.nvidia.com/t/preventing-ptxas-from-reordering-instructions/28682/13)


`bar.arrive $barnum $threadcount`, bar.arrive can be used to reset a barrier, and thus 
threadcount indicates how many threads the new barrier must wait on.






##  Low bit dtypes, quantization
https://www.youtube.com/watch?v=3qNZvvlwcCI&t=156s

Scaling factors per row cheap, that is per row, column in dot product.
But can lead to large outliers zeroing complete areas.

Scaling factors per smaller blocks more expensive without 
hardware support, but better encoding and outliers dont 
kill as many other values. Plus the required dynamic range is lower 
because the scale factor can handle more.


OCP MX Block Formats
https://en.wikipedia.org/wiki/Block_floating_point





## Polyhedral Framework 
This is a way to represent nested loops as points in a d dimensional space. 
d is the number of nested loops, loop dependencies can be represented 
by lines between the points. The faces/bounds are determined by the loop bounds 
for each loop. We can apply affine transformations,preserve the dependencies,
onto the indicies to change the processing order and significantly improve performance.
For example:
```c
for (i = 0 ; i < N ; ++i){
    for (j = 0 ; j < M; ++j){
        A[i+1][j+1] = f(A[i][j])
    }
}
```
can be visualized as a two dimensional space, there are lines 
from points $(i,j)$ to points $(i+1,j+1)$. This means if we want to parallelize 
the inner loop, in `j`, we have to synchronize after every iteration.
When we attempt to parallelize nested loops, each loop level presents 
an opportunity to distribute its iteration to different threads.
If we parallelize `i`, then each thread performs:
```c
    for (j = 0 ; j < M; ++j){
        A[i+1][j+1] = f(A[i][j])
    }
```
for `j` each thread exectues one `A[i+1][j+1] = f(A[i][j])`. We essentially 
traverse the coordinate system along one of its axis, each thread gets assigned 
one point on this axis and traverses the other axis according to some transformation.

For example parallelizing in `i` means each point on the `i` axis corresponds to one 
thread and we could try to traverse `j` with an idenity mapping i.e. `(i,j) -> (j)` 
which will obviously not work because of our inter loop dependency. 

The only valid parallelization ,that don't require synchronization, are those that follow 
the paths `(1,1)`. We seek now to find an affine transformation, that gives us a `(1,0)` or `(0,1)`
dependency. These type of dependencies are restricted to one axis, leaving the other free 
to be parallelized. In our case we can transform `(i,j) -> (i-j,j)` which yields `(0,1)` dependencies.
Although `(i,j) -> (i,j-i)` is prefectly valid as well. the first one will shift our iteration 
space to `(-M+1 : N-1)` and `(0:M-1)`, all dependencies now point perfectly up, `(0,1)`
This means we can assign each point on the `i` axis to one thread. The new loop structure is now: 
```c
for (ti = -M + 1 ; i < N ; ++i){
    for (tj = max(0,-ti) ; j < min(M-1,N-1,-ti); ++tj){
        A[ti+tj+1][tj+1] = f(A[ti+tj][tj])
    }
}
```
The array indicies can be derived by transforming `(i-j,j) -> (i,j)`, where `ti = i-j` and `tj = j`.
### TODO: Make plots

Any dependencies with a 0 in the vector can be parallelized perfectly, since this means no dependencies 
exists for that dimension. Among the class of affine transformations are scaling ones,
`(i,j,0,0) -> (a * i,b * j,i,j)`, these encompass tiling transformations on loops. 
Visually the above transformation creates 2d tiles of points of width `1/a` and height `1/b`,
the tiling is sensible, as long as no cyclic dependencies exist between the tiles. A 
sufficient condition is if all dependencies are non-negative along the dimensions
that are being tiled. 

Sensible here means, that tiles can actually be distributed among different threads.
Consider the dependencies `(1,0),(1,1),(1,-1)`, or : 
```c
for (i=0; i < N-1 ;++i){
    for (j=1; j < M-1 ; ++j ){
        A[(i+1)][j] = f(A[i][j-1],A[i][j],A[i][j+1])
    }
}
```
### TODO: Make plots

Visually a point `(i,j)`, with `i>1,j>1` has ingoing edges coming from `(i-1,j)`,`(i-1,j-1)` and 
`(i-1,j+1)`, for the outgoing edges just change the signs. This yields the dependencies `(1,0),(1,1),(1,-1)`,
tiling naively along `j` is going to create dependency chains which we cannot resolve. 
We first need to eliminate this dependency through a different transformation that eliminates 
this negative dependency. Similar to before we can use `(i,j) -> (i,i+j)`, transforming 
our dependencies from `(1,0),(1,1),(1,-1) -> (1,1),(1,2),(1,0)` allowing us to tile 
along both `ti` and `tj` where `ti = i` and `tj = i+j`. 

These transformations can be nested to create arbitrary complex patterns, for example:
`(i,j) -> (i/2 + (i+j)/2,(i+j)/2,i,i+j)` to understand what this transformation does, 
it is easiest to break it down into individual steps: 
- First we skew `(i,j) -> (i,i+j)`
- Rename to `(i2,j2)`
- Tile `(i2,j2) -> (i2/64,j2/64,i2,j2)`
- Rename to `(ti1,tj2)`
- Skew `(ti1,tj2) -> (ti1 + tj2, tj2)`

Consider for example the dependencies `(1,0),(1,1),(1,-1)` then each step transforms the 
dependencies as follows: 
- Skew: `(1,0),(1,1),(1,-1) -> (1,1),(1,2),(1,0)`
- Tiling: For the tiling we can consider two levels 
    the point level, then our dependencies stay unchanged. 
    Or the tile level, this means we consider our boundary points 
    and how the point dependencies translate into tile dependencies.
    Dependecies such as `(1,1),(1,0),(0,1)` stay unchanged as boundary 
    points will cause them to carry over into neighbouring tiles.
    Dependecies that skip points such as `(1,2)` do change since we 
    need to determine how many tiles this points into. So for 
    every boundary point we need to check how many tile dependencies we get.

    At minimum we always get 1, i.e. `(1,2)` will always result in at least 
    `(1,1)` and additionaly `(0,1)`.

### Todo need better formula for this one

So we now have dependencies `(1,1),(1,0),(0,1)`, the final skew results in
`(1,1),(1,0),(0,1) -> (2,1),(1,0),(1,1)`. Why do this ? 
Before we couldn't parallelize in either tile dimension because 
in `j` we had the `(0,1)` dependency, `j+1` depends on `j` ,meaning we have to compute 
the tiles one after the other: 
### Insert pic here
Look at the dependency going from bottom to top.
In `i` we had the `(1,0)` dependency, meaning `i+1` depends on `i`.

After the transformation we eliminated the direct dependency of 
`j+1` on `j` for fixed `i` and instead shifted that burden into the next 
iteration. This means for fixed `i` we can somewhat parallelize `j`.
### Check here again.




Dependency wise this doesn't seem to perform any meaningful transformation 
in the sense that we can now perfectly parallelize (no zero dimension), but 
it arranges our dependencies in a way where they all progress in the same direction
in the form of a wave. 

Looking 
at a visulization of this loop, draw the iteration space as a 2d grid with points at 
$(i,j) < (N,M)$, we see 























